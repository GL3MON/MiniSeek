{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn((3, 1, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_frequencies(\n",
    "    head_dim: int, seq_len: int, device: str, theta: float = 10000.0\n",
    "):\n",
    "\n",
    "    theta = 1.0 / (theta ** ((torch.arange(0, head_dim, 2).float())/head_dim)).to(device)\n",
    "    seq_idx = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    freq_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "    return freq_complex\n",
    "\n",
    "\n",
    "def apply_rotary_embeddings(x: torch.Tensor, freq_complex: torch.Tensor, device: str):\n",
    "\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freq_complex_align = freq_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "    x_rotated = x_complex * freq_complex_align\n",
    "\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_complex = precompute_theta_frequencies(\n",
    "            32, 1, device=\"cpu\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_complex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1024])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedLatentAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,dim, head_dim, latent_kv_dim, latent_q_dim, n_heads, decop_rot_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_kv_dim = latent_kv_dim\n",
    "        self.latent_q_dim = latent_kv_dim\n",
    "        self.head_dim =head_dim\n",
    "        self.decop_rot_dim = decop_rot_dim\n",
    "        self.expert_dim = latent_q_dim\n",
    "\n",
    "        self.latent_kv = nn.Linear(self.dim, latent_kv_dim, bias=False)\n",
    "        self.latent_q = nn.Linear(self.dim, latent_q_dim, bias=False)\n",
    "\n",
    "        self.query = nn.Linear(latent_q_dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.key = nn.Linear(latent_kv_dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.value = nn.Linear(latent_kv_dim, self.n_heads * self.head_dim, bias=False)\n",
    "\n",
    "        self.decop_rot_q = nn.Linear(latent_q_dim, self.n_heads * self.decop_rot_dim)\n",
    "        self.decop_rot_k = nn.Linear(self.dim, self.n_heads * self.decop_rot_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.head_dim * self.n_heads, self.expert_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, freq_complex: torch.Tensor):\n",
    "        x: torch.Tensor = x\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        cq = self.latent_q(x)\n",
    "        ckv = self.latent_kv(x)\n",
    "\n",
    "        q = self.query(cq)\n",
    "        qr = self.decop_rot_q(cq)\n",
    "\n",
    "        k = self.key(ckv)\n",
    "        kr = self.decop_rot_k(x)\n",
    "\n",
    "        v = self.value(ckv)\n",
    "        \n",
    "\n",
    "        qr = qr.view(batch_size, seq_len, self.n_heads, self.decop_rot_dim)\n",
    "        qr = apply_rotary_embeddings(qr, freq_complex, device=x.device)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        q = torch.cat((q, qr), dim=-1)\n",
    "\n",
    "        kr = kr.view(batch_size, seq_len, self.n_heads, self.decop_rot_dim)\n",
    "        kr = apply_rotary_embeddings(kr, freq_complex, device=x.device)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        k = torch.cat((k, kr), dim=-1)\n",
    "\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        print(f\"query: {q.shape} key: {k.shape} value: {v.shape}\")\n",
    "\n",
    "        att_scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(\n",
    "            self.head_dim + self.decop_rot_dim\n",
    "        )\n",
    "        att_scores = F.softmax(att_scores.float(), dim=-1).type_as(q)\n",
    "\n",
    "        print(f\"Att Scores: {att_scores.shape}\")\n",
    "\n",
    "        output = torch.matmul(att_scores, v)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        return self.out_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mla = MultiHeadedLatentAttention(head_dim=64, n_heads=64, latent_kv_dim=256, latent_q_dim=768, dim=1024, decop_rot_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: torch.Size([3, 64, 1, 96]) key: torch.Size([3, 64, 1, 96]) value: torch.Size([3, 64, 1, 64])\n",
      "Att Scores: torch.Size([3, 64, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mla(test_input, freq_complex).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
