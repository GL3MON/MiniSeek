{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn((3, 1, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_frequencies(\n",
    "    head_dim: int, seq_len: int, device: str, theta: float = 10000.0\n",
    "):\n",
    "\n",
    "    theta = 1.0 / (theta ** ((torch.arange(0, head_dim, 2).float())/head_dim)).to(device)\n",
    "    seq_idx = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    freq_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "    return freq_complex\n",
    "\n",
    "\n",
    "def apply_rotary_embeddings(x: torch.Tensor, freq_complex: torch.Tensor, device: str):\n",
    "\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freq_complex_align = freq_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "    x_rotated = x_complex * freq_complex_align\n",
    "\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_complex = precompute_theta_frequencies(\n",
    "            32, 1, device=\"cpu\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_complex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1024])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedLatentAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,dim, head_dim, latent_kv_dim, latent_q_dim, n_heads, decop_rot_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_kv_dim = latent_kv_dim\n",
    "        self.latent_q_dim = latent_kv_dim\n",
    "        self.head_dim =head_dim\n",
    "        self.decop_rot_dim = decop_rot_dim\n",
    "        self.expert_dim = latent_q_dim\n",
    "\n",
    "        self.latent_kv = nn.Linear(self.dim, latent_kv_dim, bias=False)\n",
    "        self.latent_q = nn.Linear(self.dim, latent_q_dim, bias=False)\n",
    "\n",
    "        self.query = nn.Linear(latent_q_dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.key = nn.Linear(latent_kv_dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.value = nn.Linear(latent_kv_dim, self.n_heads * self.head_dim, bias=False)\n",
    "\n",
    "        self.decop_rot_q = nn.Linear(latent_q_dim, self.n_heads * self.decop_rot_dim)\n",
    "        self.decop_rot_k = nn.Linear(self.dim, self.n_heads * self.decop_rot_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.head_dim * self.n_heads, self.dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, freq_complex: torch.Tensor):\n",
    "        x: torch.Tensor = x\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        cq = self.latent_q(x)\n",
    "        ckv = self.latent_kv(x)\n",
    "\n",
    "        q = self.query(cq)\n",
    "        qr = self.decop_rot_q(cq)\n",
    "\n",
    "        k = self.key(ckv)\n",
    "        kr = self.decop_rot_k(x)\n",
    "\n",
    "        v = self.value(ckv)\n",
    "        \n",
    "\n",
    "        qr = qr.view(batch_size, seq_len, self.n_heads, self.decop_rot_dim)\n",
    "        qr = apply_rotary_embeddings(qr, freq_complex, device=x.device)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        q = torch.cat((q, qr), dim=-1)\n",
    "\n",
    "        kr = kr.view(batch_size, seq_len, self.n_heads, self.decop_rot_dim)\n",
    "        kr = apply_rotary_embeddings(kr, freq_complex, device=x.device)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        k = torch.cat((k, kr), dim=-1)\n",
    "\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        print(f\"query: {q.shape} key: {k.shape} value: {v.shape}\")\n",
    "\n",
    "        att_scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(\n",
    "            self.head_dim + self.decop_rot_dim\n",
    "        )\n",
    "        att_scores = F.softmax(att_scores.float(), dim=-1).type_as(q)\n",
    "\n",
    "        print(f\"Att Scores: {att_scores.shape}\")\n",
    "\n",
    "        output = torch.matmul(att_scores, v)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        return self.out_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mla = MultiHeadedLatentAttention(head_dim=64, n_heads=64, latent_kv_dim=256, latent_q_dim=768, dim=1024, decop_rot_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: torch.Size([3, 64, 1, 96]) key: torch.Size([3, 64, 1, 96]) value: torch.Size([3, 64, 1, 64])\n",
      "Att Scores: torch.Size([3, 64, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "mla_out = mla(test_input, freq_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1024])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mla_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, expert_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, expert_dim),\n",
    "            nn.Linear(expert_dim, dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Expert(1024, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0509,  0.1320, -0.0619,  ...,  0.1179,  0.2809,  0.3111]],\n",
       "\n",
       "        [[ 0.1347, -0.0212, -0.1377,  ..., -0.0383,  0.4494,  0.0418]],\n",
       "\n",
       "        [[ 0.1068, -0.1219, -0.0937,  ..., -0.1031,  0.0563, -0.1148]]],\n",
       "       grad_fn=<GeluBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e(torch.randn((3, 1, 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekMoE(nn.Module):\n",
    "    def __init__(self, dim, expert_dim, n_s_experts, n_r_experts, topk):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_s_experts = n_s_experts\n",
    "        self.n_r_experts = n_r_experts\n",
    "        self.top_k = topk\n",
    "        \n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            Expert(dim, expert_dim) for _ in range(n_s_experts)\n",
    "        ])\n",
    "\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "            Expert(dim, expert_dim) for _ in range(n_r_experts)\n",
    "        ])\n",
    "\n",
    "        self.centroids = nn.Parameter(torch.randn(n_r_experts, dim))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        shared_out = torch.zeros_like(x)\n",
    "        for expert in self.shared_experts:\n",
    "            shared_out += expert(x)\n",
    "        \n",
    "        x_flat = x.view(-1, self.dim)\n",
    "        \n",
    "        affinity = torch.matmul(x_flat, self.centroids.T)\n",
    "        affinity = F.softmax(affinity, dim = -1)\n",
    "\n",
    "        topk_scores, topk_indices = torch.topk(affinity, self.top_k, dim=-1)\n",
    "        mask = torch.zeros_like(affinity)\n",
    "        mask.scatter_(-1, topk_indices, topk_scores)\n",
    "\n",
    "        routed_out = torch.zeros_like(x_flat)\n",
    "        for i in range(self.n_r_experts):\n",
    "            expert_mask = mask[:, i].unsqueeze(-1)\n",
    "            #print(f\"Expert Mask: {expert_mask}, Shape: {expert_mask.shape}\")\n",
    "            expert_out = self.routed_experts[i](x_flat)\n",
    "            routed_out += expert_mask * expert_out\n",
    "\n",
    "        routed_out = routed_out.view(batch_size, seq_len, self.dim)\n",
    "\n",
    "        return x + shared_out + routed_out, affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DeepSeekMoE(dim=1024, expert_dim=768, n_s_experts=2, n_r_experts=160, topk=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1024])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mla_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 160])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4635, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0658, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0159, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3956, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6956,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0955,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0446, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1806, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1319,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.5610, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1024])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(mla_out)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[58, 51, 27, 60]],\n",
       "\n",
       "        [[27, 39, 45, 55]],\n",
       "\n",
       "        [[11, 22, 55, 14]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 80])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
