{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn((3, 2, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_frequencies(\n",
    "    head_dim: int, seq_len: int, device: str, theta: float = 10000.0\n",
    "):\n",
    "\n",
    "    theta = 1.0 / (theta ** ((torch.arange(0, head_dim, 2).float())/head_dim)).to(device)\n",
    "    seq_idx = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    freq_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "    return freq_complex\n",
    "\n",
    "\n",
    "def apply_rotary_embeddings(x: torch.Tensor, freq_complex: torch.Tensor, device: str):\n",
    "\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freq_complex_align = freq_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "    x_rotated = x_complex * freq_complex_align\n",
    "\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_complex = precompute_theta_frequencies(\n",
    "            32, 2, device=\"cpu\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_complex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedLatentAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,dim, head_dim, latent_kv_dim, latent_q_dim, n_heads, decop_rot_dim, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_kv_dim = latent_kv_dim\n",
    "        self.latent_q_dim = latent_kv_dim\n",
    "        self.head_dim =head_dim\n",
    "        self.decop_rot_dim = decop_rot_dim\n",
    "        self.expert_dim = latent_q_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.latent_kv = nn.Linear(self.dim, latent_kv_dim, bias=False)\n",
    "        self.latent_q = nn.Linear(self.dim, latent_q_dim, bias=False)\n",
    "\n",
    "        self.query = nn.Linear(latent_q_dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.key = nn.Linear(latent_kv_dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.value = nn.Linear(latent_kv_dim, self.n_heads * self.head_dim, bias=False)\n",
    "\n",
    "        self.decop_rot_q = nn.Linear(latent_q_dim, self.n_heads * self.decop_rot_dim)\n",
    "        self.decop_rot_k = nn.Linear(self.dim, self.n_heads * self.decop_rot_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.head_dim * self.n_heads, self.dim)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(self.seq_len, self.seq_len)))\n",
    "\n",
    "\n",
    "    def forward(self, x, freq_complex: torch.Tensor, masked: bool = False):\n",
    "        x: torch.Tensor = x\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        cq = self.latent_q(x)\n",
    "        ckv = self.latent_kv(x)\n",
    "\n",
    "        q = self.query(cq)\n",
    "        qr = self.decop_rot_q(cq)\n",
    "\n",
    "        k = self.key(ckv)\n",
    "        kr = self.decop_rot_k(x)\n",
    "\n",
    "        v = self.value(ckv)\n",
    "        \n",
    "\n",
    "        qr = qr.view(batch_size, seq_len, self.n_heads, self.decop_rot_dim)\n",
    "        qr = apply_rotary_embeddings(qr, freq_complex, device=x.device)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        q = torch.cat((q, qr), dim=-1)\n",
    "\n",
    "        kr = kr.view(batch_size, seq_len, self.n_heads, self.decop_rot_dim)\n",
    "        kr = apply_rotary_embeddings(kr, freq_complex, device=x.device)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        k = torch.cat((k, kr), dim=-1)\n",
    "\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        print(f\"query: {q.shape} key: {k.shape} value: {v.shape}\")\n",
    "\n",
    "        att_scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(\n",
    "            self.head_dim + self.decop_rot_dim\n",
    "        )\n",
    "        \n",
    "        if masked:\n",
    "            att_scores = att_scores.masked_fill(self.tril[ : self.seq_len, : self.seq_len] == 0, float('-inf'))\n",
    "\n",
    "        att_scores = F.softmax(att_scores.float(), dim=-1).type_as(q)\n",
    "\n",
    "        print(f\"Att Scores: {att_scores}\")\n",
    "\n",
    "        output = torch.matmul(att_scores, v)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        return self.out_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mla = MultiHeadedLatentAttention(head_dim=64, n_heads=64, latent_kv_dim=256, latent_q_dim=768, dim=1024, decop_rot_dim=32, seq_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: torch.Size([3, 64, 2, 96]) key: torch.Size([3, 64, 2, 96]) value: torch.Size([3, 64, 2, 64])\n",
      "Att Scores: tensor([[[[1.0000, 0.0000],\n",
      "          [0.5661, 0.4339]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5755, 0.4245]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4818, 0.5182]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4349, 0.5651]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5126, 0.4874]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4213, 0.5787]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5125, 0.4875]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4649, 0.5351]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5054, 0.4946]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4825, 0.5175]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5053, 0.4947]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5191, 0.4809]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4060, 0.5940]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4745, 0.5255]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5218, 0.4782]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5261, 0.4739]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4909, 0.5091]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4998, 0.5002]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5773, 0.4227]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5490, 0.4510]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4860, 0.5140]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5197, 0.4803]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5699, 0.4301]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4771, 0.5229]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4567, 0.5433]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5041, 0.4959]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.6508, 0.3492]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4275, 0.5725]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5004, 0.4996]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5307, 0.4693]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4878, 0.5122]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5247, 0.4753]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4295, 0.5705]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5856, 0.4144]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5259, 0.4741]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5356, 0.4644]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5116, 0.4884]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4952, 0.5048]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4568, 0.5432]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5008, 0.4992]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4665, 0.5335]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5119, 0.4881]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5177, 0.4823]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5123, 0.4877]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4284, 0.5716]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5295, 0.4705]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.3767, 0.6233]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4148, 0.5852]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4379, 0.5621]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5695, 0.4305]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5126, 0.4874]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4439, 0.5561]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5423, 0.4577]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5677, 0.4323]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5155, 0.4845]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5048, 0.4952]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5518, 0.4482]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5015, 0.4985]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4750, 0.5250]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.3781, 0.6219]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4839, 0.5161]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5007, 0.4993]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4244, 0.5756]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5019, 0.4981]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000],\n",
      "          [0.5107, 0.4893]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.6232, 0.3768]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5027, 0.4973]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4993, 0.5007]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5008, 0.4992]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5435, 0.4565]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5049, 0.4951]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4927, 0.5073]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4937, 0.5063]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5101, 0.4899]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4421, 0.5579]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5192, 0.4808]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4256, 0.5744]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5247, 0.4753]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4524, 0.5476]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4236, 0.5764]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5076, 0.4924]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4989, 0.5011]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5881, 0.4119]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4735, 0.5265]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4449, 0.5551]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4590, 0.5410]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5544, 0.4456]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4966, 0.5034]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.6361, 0.3639]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5272, 0.4728]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4641, 0.5359]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4846, 0.5154]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5164, 0.4836]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5377, 0.4623]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4730, 0.5270]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5056, 0.4944]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5058, 0.4942]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4988, 0.5012]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5077, 0.4923]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4591, 0.5409]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4691, 0.5309]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4945, 0.5055]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5825, 0.4175]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5400, 0.4600]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4774, 0.5226]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4820, 0.5180]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5405, 0.4595]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4089, 0.5911]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4581, 0.5419]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5022, 0.4978]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4685, 0.5315]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4987, 0.5013]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4848, 0.5152]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4815, 0.5185]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4789, 0.5211]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5522, 0.4478]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4968, 0.5032]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4916, 0.5084]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5631, 0.4369]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4844, 0.5156]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.6141, 0.3859]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4563, 0.5437]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5264, 0.4736]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5579, 0.4421]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4841, 0.5159]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4594, 0.5406]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4931, 0.5069]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4224, 0.5776]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000],\n",
      "          [0.4447, 0.5553]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5126, 0.4874]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.3875, 0.6125]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4816, 0.5184]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5401, 0.4599]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5013, 0.4987]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5189, 0.4811]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5769, 0.4231]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5013, 0.4987]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5602, 0.4398]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4973, 0.5027]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4533, 0.5467]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4792, 0.5208]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4839, 0.5161]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4334, 0.5666]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4338, 0.5662]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5219, 0.4781]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.6020, 0.3980]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4635, 0.5365]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5319, 0.4681]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5990, 0.4010]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4819, 0.5181]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5857, 0.4143]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4507, 0.5493]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5410, 0.4590]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4730, 0.5270]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4196, 0.5804]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5595, 0.4405]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5616, 0.4384]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5631, 0.4369]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5442, 0.4558]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5085, 0.4915]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.6194, 0.3806]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4571, 0.5429]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4406, 0.5594]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4057, 0.5943]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4646, 0.5354]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5467, 0.4533]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4582, 0.5418]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5392, 0.4608]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4686, 0.5314]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4463, 0.5537]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4462, 0.5538]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4728, 0.5272]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5408, 0.4592]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4718, 0.5282]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4290, 0.5710]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4741, 0.5259]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5208, 0.4792]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5555, 0.4445]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5605, 0.4395]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5219, 0.4781]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5138, 0.4862]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5612, 0.4388]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5640, 0.4360]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4947, 0.5053]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.3841, 0.6159]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4703, 0.5297]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5490, 0.4510]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5245, 0.4755]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5391, 0.4609]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.4564, 0.5436]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.5055, 0.4945]],\n",
      "\n",
      "         [[1.0000, 0.0000],\n",
      "          [0.3922, 0.6078]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mla_out = mla(test_input, freq_complex, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mla_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0347, -0.1538,  0.2799,  ..., -0.0033,  0.3426,  0.1966]],\n",
       "\n",
       "        [[-0.0027, -0.2860, -0.0010,  ...,  0.0808,  0.2709,  0.2293]],\n",
       "\n",
       "        [[ 0.1381,  0.0145, -0.0479,  ..., -0.1867, -0.0727,  0.1298]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mla_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, expert_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, expert_dim),\n",
    "            nn.Linear(expert_dim, dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Expert(1024, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0509,  0.1320, -0.0619,  ...,  0.1179,  0.2809,  0.3111]],\n",
       "\n",
       "        [[ 0.1347, -0.0212, -0.1377,  ..., -0.0383,  0.4494,  0.0418]],\n",
       "\n",
       "        [[ 0.1068, -0.1219, -0.0937,  ..., -0.1031,  0.0563, -0.1148]]],\n",
       "       grad_fn=<GeluBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e(torch.randn((3, 1, 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekMoE(nn.Module):\n",
    "    def __init__(self, dim, expert_dim, n_s_experts, n_r_experts, topk):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_s_experts = n_s_experts\n",
    "        self.n_r_experts = n_r_experts\n",
    "        self.top_k = topk\n",
    "        \n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            Expert(dim, expert_dim) for _ in range(n_s_experts)\n",
    "        ])\n",
    "\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "            Expert(dim, expert_dim) for _ in range(n_r_experts)\n",
    "        ])\n",
    "\n",
    "        self.centroids = nn.Parameter(torch.randn(n_r_experts, dim))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        shared_out = torch.zeros_like(x)\n",
    "        for expert in self.shared_experts:\n",
    "            shared_out += expert(x)\n",
    "        \n",
    "        x_flat = x.view(-1, self.dim)\n",
    "        \n",
    "        affinity = torch.matmul(x_flat, self.centroids.T)\n",
    "        affinity = F.softmax(affinity, dim = -1)\n",
    "\n",
    "        topk_scores, topk_indices = torch.topk(affinity, self.top_k, dim=-1)\n",
    "        mask = torch.zeros_like(affinity)\n",
    "        mask.scatter_(-1, topk_indices, topk_scores)\n",
    "\n",
    "        routed_out = torch.zeros_like(x_flat)\n",
    "        for i in range(self.n_r_experts):\n",
    "            expert_mask = mask[:, i].unsqueeze(-1)\n",
    "            #print(f\"Expert Mask: {expert_mask}, Shape: {expert_mask.shape}\")\n",
    "            expert_out = self.routed_experts[i](x_flat)\n",
    "            routed_out += expert_mask * expert_out\n",
    "\n",
    "        routed_out = routed_out.view(batch_size, seq_len, self.dim)\n",
    "\n",
    "        return x + shared_out + routed_out, affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DeepSeekMoE(dim=1024, expert_dim=768, n_s_experts=2, n_r_experts=160, topk=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1024])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mla_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 160])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4635, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0658, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0159, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3956, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6956,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0955,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0446, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1806, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1319,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.5610, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1024])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "d(mla_out)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[58, 51, 27, 60]],\n",
       "\n",
       "        [[27, 39, 45, 55]],\n",
       "\n",
       "        [[11, 22, 55, 14]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 80])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
